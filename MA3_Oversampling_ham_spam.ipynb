{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNf9Erb7eUSWr1Sxiupmyo4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajdeepbanerjee-git/Issues_with_SMOTE/blob/main/MA3_Oversampling_ham_spam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we will setup faiss for vector search\n",
        "!pip install faiss-cpu\n",
        "\n",
        "# checking whether faiss is installed properly\n",
        "import faiss\n",
        "print(faiss.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGFpBA9pZUV2",
        "outputId": "cb7d761c-cf8c-4b09-ba40-5cc32e54f55c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "TrYjXD_sAvGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Please check the [notebook]( https://github.com/rajdeepbanerjee-git/Data_Augmentation_LLM/blob/main/MA2_p1_dataset_prep.ipynb) for data preparation. For ease of use, I am including the datasets used along with this repo."
      ],
      "metadata": {
        "id": "tG6DfbeJJuUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sample = pd.read_csv(\"/content/train.csv\")\n",
        "test_sample = pd.read_csv(\"/content/test.csv\")\n",
        "pool_sample = pd.read_csv(\"/content/pool.csv\")\n"
      ],
      "metadata": {
        "id": "w-svgqF-AvEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hypothesis:\n",
        "SMOTE may create data points which are actually not available in the real-world as minority class. Rather, in the higher dimensional vector space, they might be closer to the majority class.\n",
        "\n",
        "To prove this:\n",
        "- We will first keep aside a data pool which serve as our real-world data (around 40% of the total data)\n",
        "- We will do a SMOTE and see how the model performs on the added synthetic data.\n",
        "- Then to see why it performed bad, we will calculate the cosine similarity search with the data pool, take the top similar data point from the pool that is similar to the synthetic minority sample and figure out the % of data points that are closer to the majority than the minority.\n",
        "- Higher the percentage, worse is the performance of SMOTE.\n",
        "\n",
        "This analysis is motivated from the paper [\"Stop oversampling for class imbalance learning\"](https://arxiv.org/abs/2202.03579). Although, I took a simpler approach than in the paper, but the result still holds true.\n"
      ],
      "metadata": {
        "id": "ErXTHA36vKTE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial class distribution before sampling\n",
        "\n",
        "train_sample['label'].value_counts(normalize = True).round(2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "4Bxc0w2beSRl",
        "outputId": "277fe734-4633-4c16-e747-3eecda766e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "0    0.87\n",
              "1    0.13\n",
              "Name: proportion, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>proportion</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.13</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorize all the data\n",
        "vectorizer = TfidfVectorizer(min_df = 0.01)\n",
        "\n",
        "# Fit and transform the text data to create TF-IDF vectors\n",
        "train_tfidf_mat = vectorizer.fit_transform(train_sample['sms'])\n",
        "test_tfidf_mat = vectorizer.transform(test_sample['sms'])\n",
        "\n",
        "# oversample minority class\n",
        "# we want to increase the minority class by around 100 data points => ~10%, i.e, from 13% to 23%\n",
        "smote = SMOTE(sampling_strategy = 'auto', random_state=42)\n",
        "y_train = train_sample['label']\n",
        "X_res_smote, y_res_smote = smote.fit_resample(train_tfidf_mat, y_train)\n"
      ],
      "metadata": {
        "id": "arVs30fZZUe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# determine which are the synthetic samples\n",
        "\n",
        "# Convert to sets for comparison\n",
        "original_data = set(map(tuple, train_tfidf_mat.toarray()))\n",
        "resampled_data = set(map(tuple, X_res_smote.toarray()))\n",
        "\n",
        "# Find the synthetic samples (new data added by SMOTE)\n",
        "synthetic_samples = resampled_data - original_data\n",
        "synthetic_samples = np.array(list(synthetic_samples))\n",
        "\n",
        "print(\"Addeded data size:\", synthetic_samples.shape[0])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "clWNx1JeZUb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrain with augmented data - with sampling strategy 'auto'\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Initialize the Multinomial Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Train the classifier on the training data\n",
        "nb_classifier.fit(X_res_smote, y_res_smote)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = nb_classifier.predict(test_tfidf_mat)\n",
        "\n",
        "# Calculate the accuracy of the classifier\n",
        "accuracy = accuracy_score(test_sample['label'], y_pred)\n",
        "print(f\"Accuracy: {accuracy.round(2)}\")\n",
        "\n",
        "# Print the classification report\n",
        "report = classification_report(test_sample['label'], y_pred)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvNXDwNllo5t",
        "outputId": "7cf14d14-f0d7-4037-ef56-fa3055400602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.93\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.93      0.96      1156\n",
            "           1       0.68      0.94      0.79       182\n",
            "\n",
            "    accuracy                           0.93      1338\n",
            "   macro avg       0.84      0.94      0.87      1338\n",
            "weighted avg       0.95      0.93      0.94      1338\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The balanced dataset has worse results than the imbalanced one! It increased recall, but decreased precision!\n"
      ],
      "metadata": {
        "id": "i3BYEBzLnDPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transform the pool and add to faiss index to check similarity with the synthetic samples\n",
        "pool_sample_tfidf = vectorizer.transform(pool_sample['sms'])\n",
        "\n",
        "# Create the FAISS index, before that we need to convert the tfidf vectors in proper format needed for faiss\n",
        "# cosine similarity is used, can use ndexFlatIP for inner product based similarity\n",
        "pool_tfidf_emb = pool_sample_tfidf.toarray().astype('float32') # should be the embedding search space\n",
        "emb_len = pool_tfidf_emb.shape[1]\n",
        "index = faiss.IndexFlatIP(emb_len) # pass the length the embedding\n",
        "index.add(pool_tfidf_emb)  # Add embeddings to the index"
      ],
      "metadata": {
        "id": "IPyi_iaymEIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to get top k similar data from the pool, that are similar to queries (FN cases)\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_top_k(queries_dense, faiss_index, k):\n",
        "    # first change the query vector from sparse to the dense format acceptable in faiss\n",
        "    # queries_dense = query_matrix.toarray().astype('float32')\n",
        "    similar_indices = []\n",
        "    for i in tqdm(range(queries_dense.shape[0])):\n",
        "      query_vector = queries_dense[i].reshape(1, -1)\n",
        "      distances, indices = index.search(query_vector, k) # faiss index search\n",
        "      similar_indices.append({\"indices\": indices, \"distances\":distances})\n",
        "    sim_ind_df = pd.DataFrame(similar_indices)\n",
        "\n",
        "    return sim_ind_df"
      ],
      "metadata": {
        "id": "51xhEus4Au5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the function to get top k similar data indices\n",
        "sim_ind_df_pool = get_top_k(queries_dense = synthetic_samples, faiss_index = index, k = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3Ee9nbymEAb",
        "outputId": "e99ced2b-5c23-478f-fe05-93ebf83f901b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1327/1327 [00:00<00:00, 7655.79it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the counts of the labels corresponding to the most similar examples from pool data\n",
        "results = pool_sample.iloc[sim_ind_df_pool['indices']]['label'].value_counts().to_dict()\n",
        "\n",
        "# calculate the error percentage\n",
        "error = 100*results[0]/(results[0] + results[1])\n",
        "print(f\"error: {round(error, 2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrG4IzxUmD4a",
        "outputId": "29bbd0b0-f612-4129-c541-b972c3f9c0d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error: 11.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we check the similarity score with pool data - we find ~ 12% of generated samples are closer to majority than minority. This is what decreases the performance of the model."
      ],
      "metadata": {
        "id": "Woq7d-y9ptm7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XvApaPYpmDmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8JrbjGA5AutQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5UhI2hxAuMa"
      },
      "outputs": [],
      "source": []
    }
  ]
}